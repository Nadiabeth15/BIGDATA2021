{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "496d0711",
   "metadata": {},
   "source": [
    "## PREPROCESAMIENTO DE DATOS - MINERÍA DE DATOS\n",
    "**Estudiante:** Nadiabeth Diana Mallqui Apaza    \n",
    "**Código:** 171063"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61f7ba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la librería \"findspark\"\n",
    "import findspark\n",
    "# \"findspark.init()\" permite que pyspark se pueda importar como una biblioteca regular.\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b389fe36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías necesarias\n",
    "import math\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "spark = SparkSession(sc)\n",
    "sc =SparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9363cd17",
   "metadata": {},
   "source": [
    "### 1° Algoritmo : Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "de7e2a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulo BoW contabiliza las veces que aparece una palabra\n",
    "def BoW(data, columns):\n",
    "    # Convertimos nuestros datos de entrada en dataframe\n",
    "    df = spark.createDataFrame(data, columns)\n",
    "    # Mostramos\n",
    "    df.show()\n",
    "    # Realizamos el proceso del bag of words\n",
    "    bow = df.rdd\\\n",
    "        .filter(lambda x: x.Document)\\\n",
    "        .map( lambda x: x.Document.replace(',',' ').replace('.',' ').replace('-',' ').lower())\\\n",
    "        .flatMap(lambda x: x.split())\\\n",
    "        .map(lambda x: (x, 1))\n",
    "    # Mostramos las palabras y la cantidad de veces que se repiten\n",
    "    print(\"Bag of Words\")\n",
    "    print(bow.reduceByKey(lambda x,y:x+y).take(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc9e015f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------------------+\n",
      "|Month|Author|          Document|\n",
      "+-----+------+------------------+\n",
      "|  Jan|  John|This is a document|\n",
      "|  Feb|  Mary|    A book by Mary|\n",
      "|  Mar|  Luke| Newspaper article|\n",
      "|  Apr|  Mark|              null|\n",
      "+-----+------+------------------+\n",
      "\n",
      "Bag of Words\n",
      "[('book', 1), ('mary', 1), ('a', 2), ('by', 1), ('article', 1), ('this', 1), ('is', 1), ('newspaper', 1), ('document', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo Bag of Words\n",
    "data = [['Jan', 'John', 'This is a document'],\n",
    "        ['Feb', 'Mary', 'A book by Mary'],\n",
    "        ['Mar', 'Luke', 'Newspaper article'],\n",
    "        ['Apr', 'Mark', None]]\n",
    "columns = ['Month', 'Author', 'Document']\n",
    "# Hacemos uso del módulo BoW\n",
    "BoW(data,columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59090aa6",
   "metadata": {},
   "source": [
    "### 2° Algoritmo : Escalonamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d6cee1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Funciones escaladas al rango: [0.000000, 1.000000]\n",
      "+--------------+--------------+\n",
      "|      features|scaledFeatures|\n",
      "+--------------+--------------+\n",
      "|[1.0,0.1,-1.0]|     (3,[],[])|\n",
      "| [2.0,1.1,1.0]| [0.5,0.1,0.5]|\n",
      "|[3.0,10.1,3.0]| [1.0,1.0,1.0]|\n",
      "+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Librerías necesarias para el escalonamiento \n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Data de entrada\n",
    "dataFrame = spark.createDataFrame([\n",
    "    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n",
    "    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n",
    "    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n",
    "], [\"id\", \"features\"])\n",
    "\n",
    "# Ingresamos parámetros para el MinMaxScaler\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
    "\n",
    "# Calcule estadísticas resumidas y genere MinMaxScalerModel\n",
    "scalerModel = scaler.fit(dataFrame)\n",
    "\n",
    "# Cambiar la escala de cada característica al rango [mínimo, máximo]\n",
    "scaledData = scalerModel.transform(dataFrame)\n",
    "print(\"Funciones escaladas al rango: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "scaledData.select(\"features\", \"scaledFeatures\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c5193e",
   "metadata": {},
   "source": [
    "### 3° Algoritmo : Binario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8eac4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salida del binarizador con umbral = 0.500000\n",
      "+---+-------+-----------------+\n",
      "| id|feature|binarized_feature|\n",
      "+---+-------+-----------------+\n",
      "|  0|    0.1|              0.0|\n",
      "|  1|    0.8|              1.0|\n",
      "|  2|    0.2|              0.0|\n",
      "+---+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Librería Binarizer\n",
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "# DataFrame de entrada\n",
    "continuousDataFrame = spark.createDataFrame([\n",
    "    (0, 0.1),\n",
    "    (1, 0.8),\n",
    "    (2, 0.2)\n",
    "], [\"id\", \"feature\"])\n",
    "\n",
    "# Método Binarizer\n",
    "binarizer = Binarizer(threshold=0.5, inputCol=\"feature\", outputCol=\"binarized_feature\")\n",
    "\n",
    "# Mostramos en dataframe\n",
    "binarizedDataFrame = binarizer.transform(continuousDataFrame)\n",
    "\n",
    "print(\"Salida del binarizador con umbral = %f\" % binarizer.getThreshold())\n",
    "binarizedDataFrame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c3f411",
   "metadata": {},
   "source": [
    "### 4° Algoritmo : Distancia de Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb423fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de datos donde los valores hash se almacenan en la columna 'hashes'\n",
      "+---+--------------------+--------------------+\n",
      "| id|            features|              hashes|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|(6,[0,1,2],[1.0,1...|[[2.60304514E8], ...|\n",
      "|  1|(6,[2,3,4],[1.0,1...|[[4.68158287E8], ...|\n",
      "|  2|(6,[0,2,4],[1.0,1...|[[2.60304514E8], ...|\n",
      "+---+--------------------+--------------------+\n",
      "\n",
      "Mostramos la Distancia de Jaccard menor a 0.6\n",
      "+---+---+--------------------+\n",
      "|idA|idB|Distancia de Jaccard|\n",
      "+---+---+--------------------+\n",
      "|  2|  5|                 0.5|\n",
      "|  1|  5|                 0.5|\n",
      "|  0|  5|                 0.5|\n",
      "+---+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importamos la librerías a utilizar\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Ingresamos los datos A como vectores\n",
    "dataA = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),\n",
    "         (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),\n",
    "         (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),)]\n",
    "# Creamos un DataFrame con los datos anteriormente ingresados (datos A)\n",
    "dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])\n",
    "\n",
    "# De igual manera, ingresamos los datos B como vectores\n",
    "dataB = [(3, Vectors.sparse(6, [1, 3, 5], [1.0, 1.0, 1.0]),),\n",
    "         (4, Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),\n",
    "         (5, Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),)]\n",
    "# Creamos un DataFrame con los datos anteriormente ingresados (datos B)\n",
    "dfB = spark.createDataFrame(dataB, [\"id\", \"features\"])\n",
    "\n",
    "key = Vectors.sparse(6, [1, 3], [1.0, 1.0])\n",
    "\n",
    "# Clase pública MinHashLSH para distancia de Jaccard.\n",
    "# inputCol() ----> Parámetro para el nombre de la columna de entrada.\n",
    "# outputCol() ----> Parámetro para el nombre de la columna de salida.\n",
    "# numHashTables() ----> Parámetro para el número de tablas hash utilizadas en la amplificación OR de LSH.\n",
    "\n",
    "mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\n",
    "model = mh.fit(dfA)\n",
    "\n",
    "# Transformación de características\n",
    "print(\"Conjunto de datos donde los valores hash se almacenan en la columna 'hashes'\")\n",
    "model.transform(dfA).show()\n",
    "\n",
    "# Realizamos un cálculo aproximado de unión de similitud - Distancia de Jaccard \n",
    "print(\"Mostramos la Distancia de Jaccard menor a 0.6\")\n",
    "model.approxSimilarityJoin(dfA, dfB, 0.6, distCol=\"Distancia de Jaccard\")\\\n",
    "    .select(col(\"datasetA.id\").alias(\"idA\"),\n",
    "            col(\"datasetB.id\").alias(\"idB\"),\n",
    "            col(\"Distancia de Jaccard\")).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c71287",
   "metadata": {},
   "source": [
    "### 5° Algoritmo : N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "76606ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------------------------------+\n",
      "|id |Palabras                                       |\n",
      "+---+-----------------------------------------------+\n",
      "|0  |[Hola, yo, escuché, sobre, Spark]              |\n",
      "|1  |[Ojalá, Java, pudiera, usar, clases, de, casos]|\n",
      "|2  |[Modelos, Regresión, Logística, son, ordenados]|\n",
      "+---+-----------------------------------------------+\n",
      "\n",
      "Mostramos los n-gramas con n=2\n",
      "+--------------------------------------------------------------------------+\n",
      "|N-grams                                                                   |\n",
      "+--------------------------------------------------------------------------+\n",
      "|[Hola yo, yo escuché, escuché sobre, sobre Spark]                         |\n",
      "|[Ojalá Java, Java pudiera, pudiera usar, usar clases, clases de, de casos]|\n",
      "|[Modelos Regresión, Regresión Logística, Logística son, son ordenados]    |\n",
      "+--------------------------------------------------------------------------+\n",
      "\n",
      "Mostramos los n-gramas con n=3\n",
      "+---------------------------------------------------------------------------------------------+\n",
      "|N-grams                                                                                      |\n",
      "+---------------------------------------------------------------------------------------------+\n",
      "|[Hola yo escuché, yo escuché sobre, escuché sobre Spark]                                     |\n",
      "|[Ojalá Java pudiera, Java pudiera usar, pudiera usar clases, usar clases de, clases de casos]|\n",
      "|[Modelos Regresión Logística, Regresión Logística son, Logística son ordenados]              |\n",
      "+---------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importamos la librerías a utilizar\n",
    "from pyspark.ml.feature import NGram\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creamos un DataFrame con los datos ingresados\n",
    "wordDataFrame = spark.createDataFrame([\n",
    "        (0, [\"Hola\", \"yo\", \"escuché\", \"sobre\", \"Spark\"]),\n",
    "        (1, [\"Ojalá\", \"Java\", \"pudiera\", \"usar\", \"clases\", \"de\", \"casos\"]),\n",
    "        (2, [\"Modelos\", \"Regresión\", \"Logística\", \"son\", \"ordenados\"])\n",
    "    ], [\"id\", \"Palabras\"])\n",
    "\n",
    "# Mostramos el DataFRame con las palabras ingresadas\n",
    "wordDataFrame.show(truncate=False)\n",
    "\n",
    "# EJEMPLOS\n",
    "# Calculamos los n-gramas que deseamos\n",
    "print(\"Mostramos los n-gramas con n=2\")\n",
    "ngram1 = NGram(n=2, inputCol=\"Palabras\", outputCol=\"N-grams\")\n",
    "# Transformamos las palabras en un dataframe\n",
    "ngramDataFrame = ngram1.transform(wordDataFrame)\n",
    "# Mostramos el dataframe con los n-gramas respectivos\n",
    "ngramDataFrame.select(\"N-grams\").show(truncate=False)\n",
    "#-----------------------------------------------------------------\n",
    "# Calculamos los n-gramas que deseamos\n",
    "print(\"Mostramos los n-gramas con n=3\")\n",
    "ngram2 = NGram(n=3, inputCol=\"Palabras\", outputCol=\"N-grams\")\n",
    "# Transformamos las palabras en un dataframe\n",
    "ngramDataFrame = ngram2.transform(wordDataFrame)\n",
    "# Mostramos el dataframe con los n-gramas respectivos\n",
    "ngramDataFrame.select(\"N-grams\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
